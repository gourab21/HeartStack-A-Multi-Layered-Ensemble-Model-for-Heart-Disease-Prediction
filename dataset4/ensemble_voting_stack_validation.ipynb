{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"./Cardiovascular_Disease_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 14)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (640, 13)\n",
      "Shape of X_val: (160, 13)\n",
      "Shape of X_test: (200, 13)\n",
      "Shape of y_train: (640,)\n",
      "Shape of y_val: (160,)\n",
      "Shape of y_test: (200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X is your feature matrix and y is your target variable\n",
    "X = dataset.drop(columns=['target'])\n",
    "y = dataset['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training, validation, and testing sets\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    StackingClassifier,    RandomForestClassifier,     GradientBoostingClassifier,     AdaBoostClassifier,     ExtraTreesClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# Base models\n",
    "rf = RandomForestClassifier(criterion='entropy', max_features='sqrt')\n",
    "dt = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=5, splitter='random')\n",
    "gb = GradientBoostingClassifier(max_depth=4, min_samples_leaf=20)\n",
    "gnb = GaussianNB(var_smoothing=0.004328761281083057)\n",
    "svm = SVC(C=100, gamma='auto', probability=True)\n",
    "knn = KNeighborsClassifier(metric='manhattan', n_neighbors=9, weights='distance')\n",
    "lr = LogisticRegression(C=0.615848211066026, max_iter=500, penalty='l1', solver='liblinear')\n",
    "\n",
    "# Additional models\n",
    "et = ExtraTreesClassifier(n_estimators=100, criterion='entropy', max_features='sqrt')\n",
    "adb = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "xgb_clf = xgb.XGBClassifier(max_depth=4, learning_rate=0.1, n_estimators=100, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Creating a list of all classifiers\n",
    "models = {\n",
    "    \"Random Forest\": rf,    \"Decision Tree\": dt,    \"Gradient Boosting\": gb,    \"GaussianNB\": gnb,    \"SVM\": svm,    \"KNN\": knn,    \"Logistic Regression\": lr,    \"Extra Trees\": et,    \"AdaBoost\": adb,    \"XGBoost\": xgb_clf}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "es1 = [('clf1', et), ('clf2', rf), ('clf3', gb), ('clf4', dt)]\n",
    "es2 = [('clf1', et), ('clf2', rf), ('clf3', gb), ('clf4', dt)]\n",
    "es3 = [('clf1', et), ('clf2', rf), ('clf3', gb), ('clf4', dt)]\n",
    "es4 = [('clf1', et), ('clf2', rf), ('clf3', gb), ('clf4', dt)]  #gnb replaced with dt\n",
    "\n",
    "ev1 = [('clf1', rf), ('clf2', dt), ('clf3', gb), ('clf4', gnb)]\n",
    "ev2 = [('clf1', dt), ('clf2', gb), ('clf3', svm), ('clf4', adb)]\n",
    "ev3 = [('clf1', rf), ('clf2', knn), ('clf3', dt), ('clf4', gb)]\n",
    "ev4 = [('clf1', dt), ('clf2', gb), ('clf3', lr), ('clf4', svm)]\n",
    "ev5 = []\n",
    "\n",
    "\n",
    "es5 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 style=\"Color:Red\"> Stack 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for stack 1(Validation): \n",
      "Validation Accuracy: 0.9750\n",
      "Validation Precision: 0.9752\n",
      "Validation Recall: 0.9750\n",
      "Validation F1 Score: 0.9749\n",
      "False Negative Percentage for Validation Data: 1.01%\n",
      "\n",
      "\n",
      "Accuracy for stack 1(Test): \n",
      "Test Accuracy: 0.9900\n",
      "Test Precision: 0.9900\n",
      "Test Recall: 0.9900\n",
      "Test F1 Score: 0.9900\n",
      "False Negative Percentage for Test Data: 0.93%\n"
     ]
    }
   ],
   "source": [
    "# Create a VotingClassifier with base classifiers clf1, clf2, clf3, and clf4\n",
    "voting_clf1 = VotingClassifier(\n",
    "    estimators=ev1,\n",
    "    voting='soft'\n",
    ")\n",
    "# Create a StackingClassifier with base classifiers clf1, clf2, clf3, and clf4, and meta-classifier voting_clf\n",
    "sclf1 = StackingClassifier(\n",
    "    estimators=es1,\n",
    "    final_estimator=voting_clf1\n",
    ")\n",
    "\n",
    "# Fit the Stacking Classifier to the training data\n",
    "sclf1.fit(X_train, y_train)\n",
    "\n",
    "# Create a pipeline with the Stacking Classifier and the final logistic regression model\n",
    "pipeline = make_pipeline(sclf1, voting_clf1)\n",
    "\n",
    "# Use the pipeline to transform the training data into the feature space of the logistic regression model\n",
    "transformed_X_train1 = pipeline[:-1].transform(X_train)\n",
    "\n",
    "# Now you can inspect the transformed training data\n",
    "# For example, you can print the first few rows\n",
    "# print(transformed_X_train1[:5])  # Print the first 5 rows\n",
    "\n",
    "# Optionally, you can perform further analysis or visualization on this transformed data\n",
    "# For example, you can plot histograms or scatter plots to explore the distribution of features\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "def calculate_false_negative(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return fn / (fn + tp) * 100\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the function to calculate false negative percentage\n",
    "\n",
    "\n",
    "# Assuming you have defined the necessary variables and models\n",
    "\n",
    "print(\"Accuracy for stack 1(Validation): \")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions1 = sclf1.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, val_predictions1)\n",
    "print(\"Validation Accuracy: %0.4f\" % accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_val, val_predictions1, average='weighted')\n",
    "print(\"Validation Precision: %0.4f\" % precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_val, val_predictions1, average='weighted')\n",
    "print(\"Validation Recall: %0.4f\" % recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_val, val_predictions1, average='weighted')\n",
    "print(\"Validation F1 Score: %0.4f\" % f1)\n",
    "\n",
    "# Plot confusion matrix for validation data\n",
    "# cm = confusion_matrix(y_val, val_predictions1)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_val), yticklabels=np.unique(y_val))\n",
    "# plt.title(\"Confusion Matrix - Validation Data\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate false negative percentage for validation data\n",
    "false_negative_percentage = calculate_false_negative(y_val, val_predictions1)\n",
    "print(f\"False Negative Percentage for Validation Data: {false_negative_percentage:.2f}%\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Accuracy for stack 1(Test): \")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions1 = sclf1.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, test_predictions1)\n",
    "print(\"Test Accuracy: %0.4f\" % test_accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "test_precision = precision_score(y_test, test_predictions1, average='weighted')\n",
    "print(\"Test Precision: %0.4f\" % test_precision)\n",
    "\n",
    "# Calculate recall\n",
    "test_recall = recall_score(y_test, test_predictions1, average='weighted')\n",
    "print(\"Test Recall: %0.4f\" % test_recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "test_f1 = f1_score(y_test, test_predictions1, average='weighted')\n",
    "print(\"Test F1 Score: %0.4f\" % test_f1)\n",
    "\n",
    "# Plot confusion matrix for test data\n",
    "# cm = confusion_matrix(y_test, test_predictions1)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "# plt.title(\"Confusion Matrix - Test Data\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate false negative percentage for test data\n",
    "false_negative_percentage = calculate_false_negative(y_test, test_predictions1)\n",
    "print(f\"False Negative Percentage for Test Data: {false_negative_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 style=\"Color:Red\"> Stack 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for stack 2(Validation): \n",
      "Validation Accuracy: 0.9750\n",
      "Validation Precision: 0.9752\n",
      "Validation Recall: 0.9750\n",
      "Validation F1 Score: 0.9749\n",
      "False Negative Percentage for Validation Data: 1.01%\n",
      "\n",
      "\n",
      "Accuracy for stack 2(Test): \n",
      "Test Accuracy: 0.9700\n",
      "Test Precision: 0.9707\n",
      "Test Recall: 0.9700\n",
      "Test F1 Score: 0.9699\n",
      "False Negative Percentage for Test Data: 0.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a VotingClassifier with base classifiers clf1, clf2, clf3, and clf4\n",
    "voting_clf2 = VotingClassifier(\n",
    "    estimators=ev2,\n",
    "    voting='soft'\n",
    ")\n",
    "# Create a StackingClassifier with base classifiers clf1, clf2, clf3, and clf4, and meta-classifier voting_clf\n",
    "sclf2 = StackingClassifier(\n",
    "    estimators=es2,\n",
    "    final_estimator=voting_clf2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fit the Stacking Classifier to the training data\n",
    "sclf2.fit(X_train, y_train)\n",
    "\n",
    "# Create a pipeline with the Stacking Classifier and the final logistic regression model\n",
    "pipeline = make_pipeline(sclf2, voting_clf2)\n",
    "\n",
    "# Use the pipeline to transform the training data into the feature space of the logistic regression model\n",
    "transformed_X_train2 = pipeline[:-1].transform(X_train)\n",
    "\n",
    "\n",
    "# print(transformed_X_train2[:5])  # Print the first 5 rows\n",
    "\n",
    "print(\"Accuracy for stack 2(Validation): \")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions2 = sclf2.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, val_predictions2)\n",
    "print(\"Validation Accuracy: %0.4f\" % accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_val, val_predictions2, average='weighted')\n",
    "print(\"Validation Precision: %0.4f\" % precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_val, val_predictions2, average='weighted')\n",
    "print(\"Validation Recall: %0.4f\" % recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_val, val_predictions2, average='weighted')\n",
    "print(\"Validation F1 Score: %0.4f\" % f1)\n",
    "\n",
    "# Plot confusion matrix for validation data\n",
    "# cm = confusion_matrix(y_val, val_predictions2)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_val), yticklabels=np.unique(y_val))\n",
    "# plt.title(\"Confusion Matrix - Validation Data\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate false negative percentage for validation data\n",
    "false_negative_percentage = calculate_false_negative(y_val, val_predictions2)\n",
    "print(f\"False Negative Percentage for Validation Data: {false_negative_percentage:.2f}%\")\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Accuracy for stack 2(Test): \")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions2 = sclf2.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, test_predictions2)\n",
    "print(\"Test Accuracy: %0.4f\" % test_accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "test_precision = precision_score(y_test, test_predictions2, average='weighted')\n",
    "print(\"Test Precision: %0.4f\" % test_precision)\n",
    "\n",
    "# Calculate recall\n",
    "test_recall = recall_score(y_test, test_predictions2, average='weighted')\n",
    "print(\"Test Recall: %0.4f\" % test_recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "test_f1 = f1_score(y_test, test_predictions2, average='weighted')\n",
    "print(\"Test F1 Score: %0.4f\" % test_f1)\n",
    "\n",
    "# Plot confusion matrix for test data\n",
    "# cm = confusion_matrix(y_test, test_predictions2)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "# plt.title(\"Confusion Matrix - Test Data\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate false negative percentage for test data\n",
    "false_negative_percentage = calculate_false_negative(y_test, test_predictions2)\n",
    "print(f\"False Negative Percentage for Test Data: {false_negative_percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 style=\"Color:Red\"> Stack 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for stack 3(Validation): \n",
      "Validation Accuracy: 0.9812\n",
      "Validation Precision: 0.9818\n",
      "Validation Recall: 0.9812\n",
      "Validation F1 Score: 0.9812\n",
      "False Negative Percentage for Validation Data: 0.00%\n",
      "\n",
      "\n",
      "Accuracy for stack 3(Test): \n",
      "Test Accuracy: 0.9800\n",
      "Test Precision: 0.9807\n",
      "Test Recall: 0.9800\n",
      "Test F1 Score: 0.9800\n",
      "False Negative Percentage for Test Data: 0.00%\n"
     ]
    }
   ],
   "source": [
    "voting_clf3 = VotingClassifier(\n",
    "    estimators=ev3,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Create a StackingClassifier with base classifiers clf1, clf2, clf3, and clf4, and meta-classifier voting_clf\n",
    "sclf3 = StackingClassifier(\n",
    "    estimators=es3,\n",
    "    final_estimator=voting_clf3\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the Stacking Classifier to the training data\n",
    "sclf3.fit(X_train, y_train)\n",
    "\n",
    "# Create a pipeline with the Stacking Classifier and the final logistic regression model\n",
    "pipeline = make_pipeline(sclf3, voting_clf3)\n",
    "\n",
    "# Use the pipeline to transform the training data into the feature space of the logistic regression model\n",
    "transformed_X_train3 = pipeline[:-1].transform(X_train)\n",
    "\n",
    "\n",
    "# print(transformed_X_train3[:5])  # Print the first 5 rows\n",
    "\n",
    "print(\"Accuracy for stack 3(Validation): \")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions3 = sclf3.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, val_predictions3)\n",
    "print(\"Validation Accuracy: %0.4f\" % accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_val, val_predictions3, average='weighted')\n",
    "print(\"Validation Precision: %0.4f\" % precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_val, val_predictions3, average='weighted')\n",
    "print(\"Validation Recall: %0.4f\" % recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_val, val_predictions3, average='weighted')\n",
    "print(\"Validation F1 Score: %0.4f\" % f1)\n",
    "\n",
    "# # Plot confusion matrix for validation data\n",
    "# cm = confusion_matrix(y_val, val_predictions3)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_val), yticklabels=np.unique(y_val))\n",
    "# plt.title(\"Confusion Matrix - Validation Data\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate false negative percentage for validation data\n",
    "false_negative_percentage = calculate_false_negative(y_val, val_predictions3)\n",
    "print(f\"False Negative Percentage for Validation Data: {false_negative_percentage:.2f}%\")\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Accuracy for stack 3(Test): \")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions3 = sclf3.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, test_predictions3)\n",
    "print(\"Test Accuracy: %0.4f\" % test_accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "test_precision = precision_score(y_test, test_predictions3, average='weighted')\n",
    "print(\"Test Precision: %0.4f\" % test_precision)\n",
    "\n",
    "# Calculate recall\n",
    "test_recall = recall_score(y_test, test_predictions3, average='weighted')\n",
    "print(\"Test Recall: %0.4f\" % test_recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "test_f1 = f1_score(y_test, test_predictions3, average='weighted')\n",
    "print(\"Test F1 Score: %0.4f\" % test_f1)\n",
    "\n",
    "# Plot confusion matrix for test data\n",
    "# cm = confusion_matrix(y_test, test_predictions3)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "# plt.title(\"Confusion Matrix - Test Data\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate false negative percentage for test data\n",
    "false_negative_percentage = calculate_false_negative(y_test, test_predictions3)\n",
    "print(f\"False Negative Percentage for Test Data: {false_negative_percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 style=\"Color:Red\"> Stack 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for stack 4(Validation): \n",
      "Validation Accuracy: 0.9750\n",
      "Validation Precision: 0.9752\n",
      "Validation Recall: 0.9750\n",
      "Validation F1 Score: 0.9749\n",
      "False Negative Percentage for Validation Data: 1.01%\n",
      "\n",
      "\n",
      "Accuracy for stack 4(Test): \n",
      "Test Accuracy: 0.9800\n",
      "Test Precision: 0.9807\n",
      "Test Recall: 0.9800\n",
      "Test F1 Score: 0.9800\n",
      "False Negative Percentage for Test Data: 0.00%\n"
     ]
    }
   ],
   "source": [
    "voting_clf4 = VotingClassifier(\n",
    "    estimators=ev4,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "\n",
    "# Create a StackingClassifier with base classifiers clf1, clf2, clf3, and clf4, and meta-classifier voting_clf\n",
    "sclf4 = StackingClassifier(\n",
    "    estimators=ev4,\n",
    "    final_estimator=voting_clf4\n",
    "    \n",
    ")\n",
    "# Fit the Stacking Classifier to the training data\n",
    "sclf4.fit(X_train, y_train)\n",
    "\n",
    "# Create a pipeline with the Stacking Classifier and the final logistic regression model\n",
    "pipeline = make_pipeline(sclf4, voting_clf4)\n",
    "\n",
    "# Use the pipeline to transform the training data into the feature space of the logistic regression model\n",
    "transformed_X_train4 = pipeline[:-1].transform(X_train)\n",
    "\n",
    "\n",
    "# print(transformed_X_train4[:5])  # Print the first 5 rows\n",
    "\n",
    "print(\"Accuracy for stack 4(Validation): \")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "val_predictions4 = sclf4.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, val_predictions4)\n",
    "print(\"Validation Accuracy: %0.4f\" % accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_val, val_predictions4, average='weighted')\n",
    "print(\"Validation Precision: %0.4f\" % precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_val, val_predictions4, average='weighted')\n",
    "print(\"Validation Recall: %0.4f\" % recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_val, val_predictions4, average='weighted')\n",
    "print(\"Validation F1 Score: %0.4f\" % f1)\n",
    "\n",
    "# Plot confusion matrix for validation data\n",
    "# cm = confusion_matrix(y_val, val_predictions4)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_val), yticklabels=np.unique(y_val))\n",
    "# plt.title(\"Confusion Matrix - Validation Data\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate false negative percentage for validation data\n",
    "false_negative_percentage = calculate_false_negative(y_val, val_predictions4)\n",
    "print(f\"False Negative Percentage for Validation Data: {false_negative_percentage:.2f}%\")\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Accuracy for stack 4(Test): \")\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions4 = sclf4.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, test_predictions4)\n",
    "print(\"Test Accuracy: %0.4f\" % test_accuracy)\n",
    "\n",
    "# Calculate precision\n",
    "test_precision = precision_score(y_test, test_predictions4, average='weighted')\n",
    "print(\"Test Precision: %0.4f\" % test_precision)\n",
    "\n",
    "# Calculate recall\n",
    "test_recall = recall_score(y_test, test_predictions4, average='weighted')\n",
    "print(\"Test Recall: %0.4f\" % test_recall)\n",
    "\n",
    "# Calculate F1 score\n",
    "test_f1 = f1_score(y_test, test_predictions4, average='weighted')\n",
    "print(\"Test F1 Score: %0.4f\" % test_f1)\n",
    "\n",
    "# Plot confusion matrix for test data\n",
    "# cm = confusion_matrix(y_test, test_predictions4)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "# plt.title(\"Confusion Matrix - Test Data\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate false negative percentage for test data\n",
    "false_negative_percentage = calculate_false_negative(y_test, test_predictions4)\n",
    "print(f\"False Negative Percentage for Test Data: {false_negative_percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"Color: Violet\"> Combined Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Percentage for Combined Model (Validation Data): 1.01%\n",
      "Accuracy for Combined Model (Validation Data):  0.975\n",
      "Precision for Combined Model (Validation Data):  0.9751594227219332\n",
      "Recall for Combined Model (Validation Data):  0.975\n",
      "F1 Score for Combined Model (Validation Data):  0.9749166666666668\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Define the function to calculate false negative percentage\n",
    "def calculate_false_negative(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return fn / (fn + tp) * 100\n",
    "\n",
    "# Combine predictions using majority voting\n",
    "combined_val_predictions = []\n",
    "for p1, p2, p3, p4 in zip(val_predictions1, val_predictions2, val_predictions3, val_predictions4):\n",
    "    votes = Counter([p1, p2, p3, p4])\n",
    "    combined_val_predictions.append(votes.most_common(1)[0][0])\n",
    "\n",
    "# Plot confusion matrix for the combined model\n",
    "# cm_combined = confusion_matrix(y_val, combined_val_predictions)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm_combined, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_val), yticklabels=np.unique(y_val))\n",
    "# plt.title(\"Confusion Matrix - Combined Model (Validation Data)\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate and print the false negative percentage for the combined model\n",
    "false_negative_percentage_combined = calculate_false_negative(y_val, combined_val_predictions)\n",
    "print(f\"False Negative Percentage for Combined Model (Validation Data): {false_negative_percentage_combined:.2f}%\")\n",
    "\n",
    "# Calculate metrics for the combined model\n",
    "accuracy_combined = accuracy_score(y_val, combined_val_predictions)\n",
    "precision_combined = precision_score(y_val, combined_val_predictions, average='weighted')\n",
    "recall_combined = recall_score(y_val, combined_val_predictions, average='weighted')\n",
    "f1_combined = f1_score(y_val, combined_val_predictions, average='weighted')\n",
    "\n",
    "# Print metrics for the combined model\n",
    "print(\"Accuracy for Combined Model (Validation Data): \", accuracy_combined)\n",
    "print(\"Precision for Combined Model (Validation Data): \", precision_combined)\n",
    "print(\"Recall for Combined Model (Validation Data): \", recall_combined)\n",
    "print(\"F1 Score for Combined Model (Validation Data): \", f1_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative Percentage for Combined Model (Test Data): 0.00%\n",
      "Accuracy for Combined Model (Test Data):  0.99\n",
      "Precision for Combined Model (Test Data):  0.9901834862385321\n",
      "Recall for Combined Model (Test Data):  0.99\n",
      "F1 Score for Combined Model (Test Data):  0.9899919484702093\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Define the function to calculate false negative percentage\n",
    "def calculate_false_negative(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return fn / (fn + tp) * 100\n",
    "\n",
    "# Combine predictions using majority voting\n",
    "combined_test_predictions = []\n",
    "for p1, p2, p3, p4 in zip(test_predictions1, test_predictions2, test_predictions3, test_predictions4):\n",
    "    votes = Counter([p1, p2, p3, p4])\n",
    "    combined_test_predictions.append(votes.most_common(1)[0][0])\n",
    "\n",
    "# Plot confusion matrix for the combined model\n",
    "# cm_combined = confusion_matrix(y_test, combined_test_predictions)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm_combined, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "# plt.title(\"Confusion Matrix - Combined Model (Test Data)\")\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# Calculate and print the false negative percentage for the combined model\n",
    "false_negative_percentage_combined = calculate_false_negative(y_test, combined_test_predictions)\n",
    "print(f\"False Negative Percentage for Combined Model (Test Data): {false_negative_percentage_combined:.2f}%\")\n",
    "\n",
    "# Calculate metrics for the combined model\n",
    "accuracy_combined = accuracy_score(y_test, combined_test_predictions)\n",
    "precision_combined = precision_score(y_test, combined_test_predictions, average='weighted')\n",
    "recall_combined = recall_score(y_test, combined_test_predictions, average='weighted')\n",
    "f1_combined = f1_score(y_test, combined_test_predictions, average='weighted')\n",
    "\n",
    "# Print metrics for the combined model\n",
    "print(\"Accuracy for Combined Model (Test Data): \", accuracy_combined)\n",
    "print(\"Precision for Combined Model (Test Data): \", precision_combined)\n",
    "print(\"Recall for Combined Model (Test Data): \", recall_combined)\n",
    "print(\"F1 Score for Combined Model (Test Data): \", f1_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Negative Percentage for Combined Model (Test Data): 0.00%\n",
    "Accuracy for Combined Model (Test Data):  0.99\n",
    "Precision for Combined Model (Test Data):  0.9901834862385321\n",
    "Recall for Combined Model (Test Data):  0.99\n",
    "F1 Score for Combined Model (Test Data):  0.9899919484702093"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved successfully as 'final_stacking_model.pkl'!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the final trained stacking model\n",
    "joblib.dump(sclf4, 'final_stacking_model3.pkl')\n",
    "\n",
    "print(\"Final model saved successfully as 'final_stacking_model.pkl'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
